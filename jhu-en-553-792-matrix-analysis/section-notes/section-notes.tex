\documentclass[11pt]{article}

% Document settings, taken from Introduction to Algorithms (Dinitz).
\usepackage{epsfig}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amstext}
\usepackage{amsmath}
\usepackage{xspace}
\usepackage{hyperref}
\usepackage{fullpage}
\usepackage{enumitem}                     
\usepackage{titlesec}
\usepackage{amsthm}
\usepackage{natbib}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}

\titleformat*{\section}{\bfseries}
\titleformat*{\subsection}{\bfseries}
\titleformat*{\subsubsection}{\bfseries}
\titleformat*{\paragraph}{\bfseries}
\titleformat*{\subparagraph}{\bfseries}

\newcommand{\R}{\ensuremath{\mathbb R}}
\newcommand{\C}{\ensuremath{\mathbb C}}
\newcommand{\N}{\ensuremath{\mathbb N}}
\newcommand{\F}{\ensuremath{\mathbb F}}
\newcommand{\K}{\ensuremath{\mathbb K}}
\newcommand{\Z}{\ensuremath{\mathbb Z}}
\newcommand{\B}{\ensuremath{\mathcal B}}
\renewcommand{\H}{\ensuremath{\mathcal H}}
\newcommand{\EV}{\ensuremath{\mathbb E}}
\newcommand{\Var}{\text{Var}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\e}{\epsilon}
\newcommand{\E}{\exists}
\newcommand{\sse}{\subseteq}
\newcommand{\union}{\cup}
\newcommand{\ra}{\rightarrow}
\newcommand{\ceil}[1]{\ensuremath{\left\lceil#1\right\rceil}}
\newcommand{\floor}[1]{\ensuremath{\left\lfloor#1\right\rfloor}}
\newcommand{\ip}[2]{\left\langle #1, #2\right\rangle}
\DeclareMathOperator*{\argmax}{arg\,max\ }
\DeclareMathOperator*{\argmin}{arg\,min\ }

\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}{Lemma}[section]
\newtheorem{prop}{Proposition}[section]
\newtheorem{coro}{Corollary}[section]
\newtheorem{obs}{Observation}[section]

\theoremstyle{definition}
\newtheorem{defi}{Definition}[section]

\theoremstyle{remark}
\newtheorem{exm}{Example}[section]
\newtheorem{exc}{Exercise}[section]
\newtheorem{rem}{Remark}[section]
\newtheorem{question}{Question}
\newtheorem{answer}{Answer}

\setenumerate[0]{label=(\alph*)}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%% Document begins here %%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{document}

% EDIT THE FOLLOWING PARAMETERS FOR EACH ASSIGNMENT.

% NAME and COURSE TITLE + SECTION NUMBER
\noindent {\large {\bf Matrix Analysis and Linear Algebra}} \hfill {\bf Dr. Donniell Fishkind}

% PROFESSOR and HOMEWORK NUMBER
\noindent {{\bf TA: } Ronak Mehta} \hfill 
{Discussion Section Notes}

\noindent \rule[0.1in]{\textwidth}{0.4pt}

% CONTENT

\section{September 3, 2019}

\subsection{Section format}
\begin{itemize}
    \item Go over old homework solutions.
    \item Reiterate concepts from class.
    \item (Optional) Make connections to ideas from statistics or optimization.
    \item Answer any questions that you have.
\end{itemize}
{\bf Note:} I will prepare material for each section, but your questions are universally more important than whatever I prepare, so please ask them.
\subsection{Algorithm for success}
\begin{enumerate}
    \item Attend every class.
    \item Take exhaustive notes, typeset them in real-time or otherwise.
    \item View homework immediately after posting.
    \item Memorize lecture notes verbatim prior to exams.
\end{enumerate}

\subsection{Real analysis review}

\begin{defi}[Metric space]
    Let $X$ be a set. Let $d: X \times X \ra [0, \infty)$ with the following properties.
    \begin{enumerate}
        \item $d(x,y) \geq 0$ and $d(x,y) = 0 \iff x = y$ for $x, y \in X$. (Identity of indiscernables)
        \item $d(x,y) = d(y, x)$ (Symmetry)
        \item $d(x,y) \leq d(x,z) + d(z,y)$ for $x, y, z \in X$. (Triangle inequality)
    \end{enumerate}
    The tuple $(X, d)$ is called a metric space.
\end{defi}

\begin{exm}
    $X = \R$ and $d(x,y) = |x - y|$
\end{exm}
\begin{exm}
    $X = \R^n$ and $d(x,y) = ||x - y||_2 = \sqrt{\sum_{i=1}^n (x_i - y_i)^2}$, the Euclidean distance.
\end{exm}

\begin{defi}[Sequence]
    A sequence $(x_n)_{n=1}^\infty = x_1, x_2,...$ is a countably infinitely long list.
\end{defi}

\begin{defi}[Limit of a sequence]
    A sequence $(x_n)_{n=1}^\infty$, where $x_n \in (X,d)$, converges to limit $x \in (X,d)$ if $\forall \e > 0, \E N(\e):$
    \begin{align*}
        \forall n \geq N, \ d(x_n, x) < \e
    \end{align*}
\end{defi}
Note that convergence requires the limit to be in the space.
\begin{exm}
    The sequence $3, 3.1, 3.14, 3.141$ approaches $\pi$, but if our metric space was restricted to just the rational numbers $\mathbb{Q}$, then we would not call this a convergent sequence.
\end{exm}
\begin{exm}
    Let $x_n = \frac{1}{n}$. This sequence has limit $x = 0$.
    \begin{proof}
    Given any $\e$, let $N = \ceil{\frac{1}{\e}}$. Then, for $n \geq N:$
    \begin{align*}
        d(x_n, x) &= \left|\frac{1}{n} - 0\right| = \frac{1}{n} \leq \frac{1}{N} \leq \frac{1}{\tfrac{1}{\e}} = \epsilon
    \end{align*}
    \end{proof}
\end{exm}
\begin{defi}[Cauchy sequence]
    A sequence $(x_n)_{n=1}^\infty$, where $x_n \in (X,d)$, is Cauchy if $\forall \e > 0, \E N(\e):$
    \begin{align*}
        \forall k,l \geq N, \ d(x_k, x_l) < \e
    \end{align*}
\end{defi}
\begin{exm}
    The sequence $x_n = \frac{1}{n}$ is Cauchy. 
    \begin{proof}
    Given any $\e$, let $N = \ceil{\frac{2}{\e}}$. Then, for $k,l \geq N:$
    \begin{align*}
        d(x_k, x_l) &= \left|\frac{1}{k} - \frac{1}{l}\right| \leq \frac{1}{k} + \frac{1}{l} \leq \frac{2}{N} \leq \frac{2}{\tfrac{2}{\e}} = \epsilon
    \end{align*}
    \end{proof}
\end{exm}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/limit.png}
    \caption{$a$ denotes the limit in this sequence, $N_0$ denotes $N(\epsilon_0)$ for some $\epsilon_0$, and $N_1 = N(\epsilon_1) < N_0$ for some $\epsilon_1 < \epsilon_0$.}
\end{figure}
\begin{exc}
    Prove that a sequence converges $\implies$ the sequence is Cauchy.
\end{exc}
\begin{defi}
    A metric space $(X,d)$ is complete if every Cauchy sequence converges.
\end{defi}

\begin{defi}[Open ball]
    An open $\e$-ball about $c$ is the set $B_{\e}(c) = \{x : d(x,c) < \e\}$.
\end{defi}
\begin{defi}[Accumulation point]
    Let $A \sse X$. Point $a$ is an accumulation point of $A$ if 
    \begin{align*}
        \forall \e > 0 \ \E x \in A: x \neq a \text{ and } x \in B_{\e}(a)
    \end{align*}
    In other words, every open $\e$-ball about $a$ contains a point from $A$ that is different from $a$.
\end{defi}
\begin{exm}
    Let $X = \R$. The set $A = [0, 1)$ has accumulation point 1, as every interval $B_{\e}(1) = (1 - \e, e + \e)$ contains a point in $[0,1)$.
\end{exm}
\begin{defi}[Open set]
    A set $A \sse X$ is called open if for every $x \in A$, $\E \e>0 : B_{\e}(x) \sse A$. In other words, for every point in $A$, a small enough open ball about that point is also in $A$.
\end{defi}
\begin{exm}
    Let $X = \R$. The set $A = (0, 1)$ open.
    \begin{proof}
    Formally, let $x \in (0,1)$. Let $\e = \min\{x, 1-x\}$. Then, $B_{\e}(x) \sse (0,1)$.
    \end{proof}
\end{exm}
\begin{exm}
    Any open ball is open.
    \begin{proof}
    Using the notation from the figure below, let $A = B_r(x)$ be an open ball in $(X,d)$. Choose $y \in A$. Let $\e = r - d(x,y)$. To show that $B_{\e}(y) \sse B_r(x)$, take any point $z \in B_{\e}(y)$. By construction, we have:
    \begin{align*}
        d(y, z) < \e = r - d(x,y) \implies d(y, z) + d(x, y) < r
    \end{align*}
    To show that $z \in B_r(x)$, we bound $d(x,z)$ by $r$.
    \begin{align*}
        d(x,z) \leq d(y, z) + d(x, y) < r
    \end{align*}
    \end{proof}
\end{exm}
\clearpage
\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{figures/open_ball.png}
    \caption{An open $r$-ball about $x$. Dotted lines typically denote the unincluded boundary of the set.}
\end{figure}
\begin{defi}[Closed set]
    A set $A$ is closed if it contains all of its accumulation points.
\end{defi}
\begin{exm}
    As seen above, $[0,1)$ has (only) accumulation point 1. Thus the set $[0,1]$ is closed.
\end{exm}
\begin{thm}
    $A$ is open if and only if $A^c$ is closed.
\end{thm}
\begin{rem}
    In $\R$, the sets $\R$ and $\emptyset = \{\}$ are both open and closed.
\end{rem}
\begin{rem}
    A finite set $A = \{x_1, x_1, ..., x_n\}$ is closed. With distinct elements, no point is an accumulation point, thus all are contained.
\end{rem}
\begin{thm}
    The following hold in $(X,d)$.
    \begin{enumerate}
        \item An arbitrary number of unions of open sets is open.
        \item A finite number of intersections of open sets is open.
        \item A finite number of unions of closed sets is closed.
        \item An arbitrary number of intersections of closed sets is closed.
    \end{enumerate}
\end{thm}
\begin{exm}
    Let $A_n = (-\frac{1}{n}, \frac{1}{n})$. Each $A_n$ is open. However, $A = \bigcap_{n=1}^\infty A_n = \{0\}$ which is not open.
\end{exm}
\begin{exm}
    Let $A_n = \{\frac{1}{n}\}$. Each $A_n$ is closed. However, $A = \bigcup_{n=1}^\infty A_n = \{\frac{1}{n} : n = 1, 2, ..\}$, which has accumulation point 0, and is thus not closed. 
\end{exm}

\section{September 10, 2019}

\subsection{Real analysis review, cont'd}

\begin{defi}[Open cover]
    An open cover of set $A$ is a (possibly infinite) collection of open sets $\mathcal{S}$ such that $A \sse \bigcup_{O \in \mathcal{S}} O$. A subcover is a subset of $\mathcal{S}$ that is still a cover for $A$
\end{defi}
\begin{defi}[Compact set]
    The following are equivalent statements.
    \begin{enumerate}
        \item A set $A$ is compact.
        \item Every open cover of $A$ has a finite subcover.
        \item Every sequence $x_1, x_2,...$ with $x_n \in A$ has subsequence that converges to $x \in A$.
    \end{enumerate}
\end{defi}
\clearpage
\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{figures/finite_subcover.png}
    \caption{The brown ellipse represents a set, and the blue ellipses represent an open cover. Note that compactness does not require the existence of an open cover, but for every given (even infinite) open cover, one can extract a finite subcover.}
\end{figure}
These definitions can be difficult to verify. In $\R^n$, there is a simpler characterization.
\begin{defi}[Boundedness]
    A set $A$ is bounded if there exists a point $c \in X$ with finite radius $r$ such that $A \sse B_r(c)$. In other words, $A$ is bounded if an open ball can contain it fully.
\end{defi}
\begin{thm}[Heine-Borel]
    $A \in \R^n$ is compact if and only if it is closed and bounded.
\end{thm}
\clearpage
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/compact.png}
    \caption{The shaded regions represent elements of the set, with solid and dotted boundaries denoting inclusion and exclusion, respectively. The top-left and bottom-right are compact sets in $\R^2$.}
\end{figure}
\begin{exm}
    In $\R$, a closed interval $[a, b]$ is compact.
\end{exm}
\begin{exm}
    In $\R^n$, the unit sphere $A = \{x : ||x||_2 = 1\}$ is compact (where $||x||_2 = \sqrt{\sum_{i=1}^n x_i^2}$).
\end{exm}
\begin{defi}[Continuity]
    Let $f: (X, d_X) \rightarrow (Y, d_Y)$. $f$ is continuous at $x_0$ if for every $\e > 0$, there is a $\delta(\e, x_0)$ such that $\forall x \in X$:
    \begin{align*}
        d(x, x_0) < \delta \implies d(f(x), f(x_0)) < \e
    \end{align*}
    $f$ is continuous if it is continuous at all $x_0 \in X$.
\end{defi}
The definition of continuity captures the notion that small changes in $x$ should result in small changes in $f(x)$. Specifically, the change in $f(x)$ should be made arbitrarily small by controlling the change in $x$. Note that $\delta$ depends on both $\e$ and $x_0$. 
\clearpage
\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{figures/continuous.png}
    \caption{This function is continuous at $x = 2$.}
\end{figure}
\begin{defi}[Uniform continuity]
    Let $f: (X, d_X) \rightarrow (Y, d_Y)$. $f$ is uniformly continuous if for every $\e > 0$, there is a $\delta(\e)$ such that $\forall x_0, x_1 \in X$:
    \begin{align*}
        d(x_0, x_1) < \delta \implies d(f(x_0), f(x_1)) < \e
    \end{align*}
\end{defi}
Note that the dependence of $\delta$ on the point in $X$ is not gone.
\clearpage
\begin{figure}
    \centering
    \includegraphics[width=0.494\linewidth]{figures/uniform.png}
    \includegraphics[width=0.494\linewidth]{figures/not_uniform.png}
    \caption{A pictorial characterization of continuity whether a ring of height $2\e$ and width $2\delta$ could slide along the entire function, without turning, for every $\e$. If $\delta$ must change as this happens, the the function is only continuous. In the right example, $\delta$ must get smaller as $x$ gets large in order for the ring to continue sliding. In the case that $\delta$ can stay constant, as in the left example, then the function is uniformly continuous.}
    \label{fig:cont}
\end{figure}

\section{September 17, 2019}

In response to questions from last time:
\begin{exc}
    Show that compactness implies closure in a metric space.
\end{exc}
\begin{exm}
    Show that compactness implies boundedness in a metric space.
    \begin{proof}
        Let $(X,d)$ be the metrix space, and $A$ be the compact set of interest. Chose any $x_0 \in X$, and write $\mathcal{S} = \{B_r(x_0) : r > 0\}$. Clearly, $A \subset \mathcal{S}$, and $\mathcal{S}$ is an open cover of $A$. Thus, there exists a finite subcover
        \begin{align*}
            F = \{B_{r_1}(x_0), ..., B_{r_p}(x_0)\}
        \end{align*}
        Take $r = \max_{i=1,...,p} r_i$, and $A \sse B_r(x_0)$.
    \end{proof}
\end{exm}
\begin{exc}
    Given an example of a metric space that is closed and bounded, but not compact. Hint: Use the discrete metric. That is,
    \begin{align*}
        d(x,y) = \begin{cases}
        0 \text{ if } x = y\\
        1 \text{ if } x \neq y
        \end{cases}
    \end{align*}
\end{exc}
\begin{exm}
    Prove that $f(x) = \frac{1}{x}$ on $x > 0$ is not uniformly continuous.
    \begin{proof}
        Let $\e = 1$. For any $\delta$, we must choose $x_0$ and $x_1$ such that $|x_0 - x_1| < \delta$ does imply that $|f(x_0) - f(x_1)| < 1$.
        \begin{align*}
            |f(x_0) - f(x_1)| = \frac{\delta}{x_0 x_1}
        \end{align*}
        Letting $x_1 = x_0 + \frac{\delta}{2}$.
        \begin{align*}
            |f(x_0) - f(x_1)| = \frac{\delta}{x_0 (x_0 + \frac{\delta}{2})}
        \end{align*}
        Choosing $x_0$ small enough can make this quantity larger than $\e = 1$.
    \end{proof}
\end{exm}

Continuing with the review, there are many useful properties that result from a continuous function on a compact set.
\begin{thm}
    Let $f: (X,d_X) \rightarrow (\R, d_Y)$ be continuous over compact set $X$. Then:
    \begin{enumerate}
        \item $f$ is uniformly continuous.
        \item $f(X) = \{f(x) : x \in X\}$ is compact. 
        \item $f$ achieves $\max_{x \in X} \{f(x)\}$ and $\min_{x \in X} \{f(x)\}$, as in there exists $x^*_{\text{max}}, x^*_{\text{min}} \in X$ such that
        \begin{align*}
            f(x^*_{\text{min}}) \leq f(x) \leq f(x^*_{\text{max}})
        \end{align*}
        for all $x$.
        \item Let $\lim_{n \rightarrow \infty} x_n = x$, where $x_n, x \in X$. Then
        \begin{align*}
            \lim_{n \rightarrow \infty} f(x_n) = f\left(\lim_{n \rightarrow \infty} x_n\right) = f(x)
        \end{align*}
    \end{enumerate}
\end{thm}
\begin{exm}
    Let $f: (X,d_X) \rightarrow (Y, d_Y)$ and $g: (Y,d_Y) \rightarrow (Z, d_Z)$ both be continuous functions. Show that the composition $f \circ g$ is continuous.
    \begin{proof}
        Given any $x_0 \in X$ and any $\e > 0$, let $y_0 = f(x_0)$. Choose $\delta_g(\e, y_0)$ such that:
        \begin{align*}
            d_Y(y_0,y) < \delta_g \implies d_Z(g(y_0), g(y)) < \e
        \end{align*}
        Then choose $\delta_f(\delta_g, x_0)$ such that:
        \begin{align*}
            d_X(x_0,x) < \delta_f \implies d_Y(f(x_0),f(x)) < \delta_g
        \end{align*}
        Thus
        \begin{align*}
             d_X(x_0,x) < \delta_f \implies d_Y(f(x_0),f(x)) < \delta_g \implies d_Z(g(f(x_0)), g(f(x))) < \e
        \end{align*}
    \end{proof}
\end{exm}
\begin{exc}
    Give a function that is bounded, i.e. there is some $B \geq 0$ such that $|f(x)| \leq B$ for all $x \in X$, and continuous, but not uniformly continuous.
\end{exc}
\begin{defi}[Lipshitz continuity]
    A function $f: (X,d_X) \rightarrow (Y, d_Y)$ is called Lipshitz continuous with Lipshitz constant $L$ for all $x_0, x_1 \in X$:
    \begin{align*}
        d_Y(f(x_0), f(x_1)) \leq L \cdot d_X(x_0, x_1)
    \end{align*}
\end{defi}
This means that the changes in $f(x)$ are sublinear in the changes in $x$. The constant $L$ ``quantifies" the continuity of $f$. 
\begin{obs}
    Lipshitz continuity implies uniform continuity.
    \begin{proof}
        Given any $e > 0$, let $\delta = \frac{\e}{L}$.
        \begin{align*}
            d_Y(f(x_0), f(x_1)) \leq L \cdot d_X(x_0, x_1) \leq L \cdot \frac{\e}{L} = \e
        \end{align*}
    \end{proof}
\end{obs}

That concludes the review. I recommend doing problems from \citet{aksoy} to keep your skills up before reaching Chapter 5.

Pointers for Homework 1:
\begin{enumerate}
        \item Similarity implies sameness of rank, spectrum/charactaristic polynomial and determinant. None of the reverse implications hold.
        \item Same eigenvectors does not imply similarity. What does it imply (assuming there are $n$ of them that are linearly independent)?
        \item What can you say about a matrix with distinct eigenvalues?
        \item What can you say about the eigenvalues of diagonal/triangular matrix?
        \item A matrix is invertible - what can you say about its eigenvalues?
\end{enumerate}

\section{September 24, 2019}

{\it Homework 1 solutions.}

\section{October 1, 2019}

{\it Homework 2 solutions.}

\section{October 8, 2019}

{\it Copies of Homework 3 solutions distributed.}\\

When I say ``application", I mean that these topics are not tested in the course, but topics mostly from mathematical data science that are interesting and make use of material from the course.

\subsection*{Application: Projection Matrices} 

$A^2 = A \in M_n(\R)$. What can you say about this matrix? Given the additional information that $A = A^T$, what else can be said?

The matrix has annihilating polynomial $p(t) = t(t-1)$, therefore any eigenvalues $\lambda \in \{0,1\}$. $q_A$ divides this polynomial, so $q_A$ is the product of distinct linear factors, implying that $A$ is diagonalizable. In fact $Ax = SDS^{-1}x$ is a projection of $x$ onto the subspace spanned by the eigenvectors of $A$ associated with eigenvalue 1. This happens in three steps.
\begin{enumerate}
    \item $y_1 = S^{-1}x$ gives the coordinates of $x$ in the basis described by the columns of $A$.
    \item $y_2 = Dy_1$ scales the components by 0 or 1, eliminating certain dimensions and keeping other.
    \item $y_3 = Sy_2$ brings us back into the original basis.
\end{enumerate}
If $A = A^T$, then it is real orthogonally diagonalizable, with the eigenvectors forming an orthonormal basis. This idea is depicted in Figure \ref{fig:proj}. The transformation $A$ is called a projection matrix. The next topic will make use of ideas from Chapters 0 through 4.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.6\linewidth]{figures/projection.png}
    \caption{Here ${\bf x} \in \R^3$ is the vector of interest and ${\bf P} \in M_3(\R)$ is the projection matrix. If we ${\bf P} = SDS^{-1}$ let ${\bf A}$ be the $M_{3,2}(\R)$ matrix that column binds the eigenvectors of ${\bf P}$ associated to eigenvalue 1, then the subspace column space of ${\bf A}$.}
    \label{fig:proj}
\end{figure}

\subsection*{Application: numerical optimization}
\label{sec:opti}

Let $f: \R^d \rightarrow \R$ be a twice-differentiable function, with
\begin{align*}
    \nabla f(x) &=: g(x) \in \R^d\\
    \nabla^2 f(x) &=: H(x) \in M_d(\R)
\end{align*}
\begin{defi}[Local minimum]
    $x^*$ is a local minimum of $f$ if there exists an $\e > 0$ such that for all $x \in B_{\e}(x)$, $f(x^*) \leq f(x)$.
\end{defi}

Our goal is to find such a local minimum, and we write this problem as
\begin{align*}
    \min_{x \in \R^d} f(x)
\end{align*}

There are many ways to go about this, but one such was is to iteratively propose candidate solutions $x_1, x_2, ...$ that get ``closer" to a local minimum $x^*$. A subset of these methods is line-search methods, where at iterate $x_t$, we choose a search direction $p \in \R^d$ and step size $\alpha \in [0, 1)$, and let $x_{t+1} = x_t + \alpha p$. For example, in gradient descent, we let $p = - g(x_t)$, as in Figure \ref{fig:grad}.
\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/grad.png}
    \caption{The center of the contour plot is the local minimum.}
    \label{fig:grad}
\end{figure}

Some basic optimality conditions:
\begin{align*}
    \text{$x^*$ is a local minimum.} &\implies g(x^*) = 0 \text{ and  $H(x)$ is P.S.D.}\\
    g(x^*) = 0\text{ and  $H(x)$ is P.D.} &\implies \text{$x^*$ is a local minimum.}
\end{align*}
While we will not prove them, we will attempt to understand them via the Hessian matrix $H(x^*)$. Taylor expand $f(x_0 + \alpha p)$ about $x_0$.
\begin{align*}
    f(x_0 + \alpha p) = f(x_0) + \alpha g(x_0)^T p + \frac{1}{2} \alpha^2 p^T H(x_0) p + o(\alpha^2)
\end{align*}
Assume that we are at a stationary point $x_0$, i.e. $g(x_0) = 0$. Note that $H = H^T$ for any $x$, thus $H$ is real orthogonally diagonalizable. Let $u_1, ..., u_d$ be the eigenvectors of $H(x_0)$. Start by considering $p = u_k$.
\begin{align*}
    f(x_0 + \alpha u_k) &= f(x_0) + \alpha g(x_0)^T u_k + \frac{1}{2} \alpha^2 u_k^T H(x_0) u_k + o(\alpha^2)\\
    \implies f(x_0 + \alpha u_k) - f(x_0) &= \frac{1}{2} \alpha^2 \lambda_k + o(\alpha^2)
\end{align*}
where $\lambda_k$ is the eigenvalue associated to $u_k$. It's clear that for small $\alpha$, if $\lambda_k$ is positive, then the function will increase, where as for negative $\lambda_k$, the function will decrease. For $\lambda_k = 0$, the higher-order terms determine the sign of the change in function value.

Now consider arbitrary search direction $p$. Because $H = H^T$, the eigenvectors of $H$ for an othonormal basis for $\R^d$, and we can write
\begin{align*}
    p &= UU^Tp\\
    &= (u_1^T p) u_1 + ... + (u_d^T p) u_d\\
    &= \sum_{k=1}^d (u_k^T p) u_k
\end{align*}
Then, the the Taylor expansion gives us
\begin{align*}
    f(x_0 + \alpha u_k) - f(x_0) &= \frac{1}{2} \alpha^2 \sum_{k=1}^d (u_k^T p)^2 \lambda_k + o(\alpha^2)
\end{align*}
Thus, the change in the function is a weighted sum of the eigenvalues of $H(x_0)$, weighted by how parallel the search direction is with the associated eigenvector. If all $\lambda_k > 0$ (i.e. $H(x_0)$ is P.D.) then there is nothing to worry about, and every direction will increase the function value for small enough $\alpha$. This is just another way of saying that we are at a local minimum. However, any negative eigenvalue reveals a descent direction, and if an eigenvalue is 0, then the higher-order term $o(\alpha^2)$ could still decrease the function (which is why $H(x_0)$ P.S.D. is not sufficient). 

Finally, we answer the following question: what is the interpretation of $D$? Assume that $f(x)$ is a quadratic form, in that $f(x) = b + g^Tx + \frac{1}{2}x^T H x$ for $b \in \R$,  $x, g \in \R^d$, and $H \in M_d(\R)$ symmetric (why can we make $H$ symmetric without loss of generality?). Let $f_U(x) = f(U^T x)$.
\begin{align*}
    \nabla^2 f_U(x) &= U^T \nabla^2 f(U^T x) U\\
    &= U^T H U\\
    &= D
\end{align*}
For a quadratic function, letting $H = UDU^T$, $D$ is the Hessian of the function evaluated in a rotated basis.

Another important point is that eigenvalues of large magnitude have eigenvectors which are direction of steep increase or decrease, as suggested by the Taylor expansion. We will come back to this point when we discuss condition number, specifically how different ratios of eigenvalues affects the success of optimization algorithms. 

As a final message: in applications related to mathematical data science, when we see (say square) matrices, if you gain anything from this class it will be the willingness and ability to answer the following two questions.
\begin{enumerate}
    \item If the matrix is symmetric (Hermitian, normal), what is the  interpretation of its eigenvalues and eigenvectors? What is the interpretation of the diagonal matrix $D$?
    \item Is the matrix full-rank, low-rank, or approximately low-rank? What are the implications of each case? What is its condition number, and what are its implications?
\end{enumerate}

By approximately low-rank, we mean eigenvalues (or more generally, singular values) small in magnitude. The second question we will be able to attack more after Chapter 7 and 5, but the first we can start understanding now. 

\section{October 15, 2019}

\subsection*{Announcements}

Exam 1 just passed, and I unfortunately do not have the answer to the following questions.
\begin{itemize}
    \item I got grade $x$ on Exam 1, will I still be able to get grade $y$ in the class?
    \item Are Exams 2 and 3 easier than Exam 1?
\end{itemize}
The first question I will direct to Dr. Fishkind, while for the second, the best answer I can offer is that I looked at the exam scores from last years, and students performed statistically significantly better on Exam 2 and 3 than on Exam 1. However, operationally, the average was 4 points higher on Exam 2 and 8 points higher on Exam 3, so take that as you will. It is likely that students' expectations are better calibrated for future exams, rather than them being easier.

\subsection*{Some chapter 4 results}

\begin{thm}
    Let $A \in M_n$ be normal. Let $F(A) = \{\frac{x^* A x}{x^* x} : x \in \C^n \backslash \{0\}\}$. Then $F(A) = \mathcal{H}(A)$, where $\mathcal{H}(\cdot)$ denotes the convex hull.
\end{thm}

\begin{thm}[Rayleigh-Ritz]
    Let $A \in M_n$ be Hermitian, with (orthonormal) eigenvectors $u_1, ..., u_n$, associated with $\lambda_1 \leq ... \leq \lambda_n$. For $k = 1, ..., n$, we have:
    \begin{align*}
        \min_{\substack{x \neq 0 \\ x \perp u_1 ,..., u_{k-1}}} \frac{x^* A x}{x^* x} &= \lambda_k\\
        \max_{\substack{x \neq 0 \\ x \perp u_{k+1} ,..., u_n}} \frac{x^* A x}{x^* x} &= \lambda_k
    \end{align*}
\end{thm}
\begin{proof}
    We'll prove the maximum case. Represent any such $x$ as
    \begin{align*}
        x = \delta_{1} u_{1} + ... + \delta_k u_k = \sum_{i=1}^k \delta_i u_i
    \end{align*}
    with not all $\delta_i$ equal to 0. Let $A = UDU^*$ be the unitary diagonalization.
    \begin{align*}
        \max_{\substack{x \neq 0 \\ x \perp u_{k+1} ,..., u_n}} \frac{x^* A x}{x^* x} &= \max_{\delta_1, ..., \delta_k} \frac{(\sum_{i=1}^k \delta_i u_i)^* U D U^* \sum_{i=1}^k \delta_i u_i}{(\sum_{i=1}^k \delta_i)^* (\sum_{i=1}^k \delta_i)}\\
        &= \max_{\delta_1, ..., \delta_k} \frac{(\sum_{i=1}^k \overline{\delta_i} u_i^* U) D (\sum_{i=1}^k \delta_i U^* u_i)}{\sum_{i=1}^k \overline{\delta_i} \delta_i}\\
        &= \max_{\delta_1, ..., \delta_k} \frac{\begin{bmatrix}
            \overline{\delta_1} & \ldots & \overline{\delta_k} & 0 & \ldots & 0
        \end{bmatrix}
        \begin{bmatrix}
            \lambda_1 & & & & &\\
            & \ddots & & & &\\
            & & \lambda_k & & &\\
            & & & \lambda_{k+1} & &\\
            & & & & \ddots &\\
            & & & & & \lambda_n
        \end{bmatrix}
        \begin{bmatrix}
            \delta_1 \\ \vdots \\ \delta_k \\ 0 \\ \vdots \\ 0
        \end{bmatrix}}{\sum_{j=1}^k |\delta_j|^2}\\
        &= \max_{\delta_1, ..., \delta_k} \sum_{i=1}^k \frac{|\delta_i|^2}{\sum_{j=1}^k |\delta_j|^2} \lambda_i\\
        &= \lambda_k
    \end{align*}
\end{proof}

\begin{thm}[Courant-Fischer]
    Let $A \in M_n$ be Hermitian. Then
    \begin{align*}
        \max_{y_1 ,..., y_{k-1}} \phi_{\min}(y_1, ..., y_{k-1}) = \max_{y_1 ,..., y_{k-1}} \min_{\substack{x \neq 0 \\ x \perp y_1 ,..., y_{k-1}}} \frac{x^* A x}{x^* x} &= \lambda_k\\
        \min_{y_{k+1} ,..., y_n} \phi_{\max}(y_{k+1}, ..., y_n) = \min_{y_{k+1} ,..., y_n} \max_{\substack{x \neq 0 \\ x \perp y_{k+1} ,..., y_n}} \frac{x^* A x}{x^* x} &= \lambda_k
    \end{align*}
\end{thm}
\begin{proof}
    We show the first equality. Noting that $y_1, ..., y_{k-1}$ are not necessarily linearly independent, and letting $u_1, ..., u_k, ..., u_n$ be the eigenvectors of $\lambda_1 \leq ... \leq \lambda_k \leq ... \leq \lambda_n$, we have
    \begin{align*}
        \text{dim} \ \text{span}\{y_1, ..., y_{k-1}\} &\leq k - 1\\
        \implies \text{dim} \ \text{span}\{y_1, ..., y_{k-1}\}^{\perp} &\geq n - (k - 1) = n - k + 1\\
        \text{dim} \ \text{span}\{u_1, ..., u_k\} &= k\\
    \end{align*}
    Thus
    \begin{align*}
        \text{dim} \ \text{span}\{y_1, ..., y_{k-1}\}^{\perp} + \text{dim} \ \text{span}\{u_1, ..., u_k\} \geq n + 1
    \end{align*}
    So the intersection $\text{span}\{y_1, ..., y_{k-1}\}^{\perp} \cap \text{span}\{u_1, ..., u_k\}$ must have some nonzero vectors. Let $w \neq 0$ be one such vector. We have
    \begin{align*}
        w \perp u_{k+1}, ..., u_n,
    \end{align*}
    and so by Rayleigh-Ritz,
    \begin{align*}
        \frac{w^* A w}{w^* w} \leq \lambda_k \implies \min_{\substack{w \neq 0 \\ w \perp y_1 ,..., y_{k-1} \\ w \perp u_{k+1}, ..., u_n}} \frac{w^* A w}{w^* w} \leq \lambda_k
    \end{align*}
    This also means that
    \begin{align*}
        \phi_{\min}(y_1, ..., y_{k-1}) = \min_{\substack{x \neq 0 \\ x \perp y_1 ,..., y_{k-1}}} \frac{x^* A x}{x^* x} \leq \min_{\substack{w \neq 0 \\ w \perp y_1 ,..., y_{k-1} \\ w \perp u_{k+1}, ..., u_n}} \frac{w^* A w}{w^* w} \leq \lambda_k
    \end{align*}
    and thus we have found an upper bound for $\phi_{\min}(y_1, ..., y_{k-1})$. Setting $y_1 = u_1, ..., y_{k-1} = u_{k-1}$, we achieve this upper bound, and thus
    \begin{align*}
        \max_{y_1 ,..., y_{k-1}} \phi_{\min}(y_1, ..., y_{k-1}) = \max_{y_1 ,..., y_{k-1}} \min_{\substack{x \neq 0 \\ x \perp y_1 ,..., y_{k-1}}} \frac{x^* A x}{x^* x} &= \lambda_k\\
    \end{align*}
    as desired. As for the second equality (which is entirely analogous), we have
    \begin{align*}
        \text{dim} \ \text{span}\{y_{k+1}, ..., y_n\} &\leq n - k\\
        \implies \text{dim} \ \text{span}\{y_{k+1}, ..., y_n\}^{\perp} &\geq n - (n - k) = k\\
        \text{dim} \ \text{span}\{u_k, ..., u_n\} &= n - k + 1\\
    \end{align*}
    Thus
    \begin{align*}
        \text{dim} \ \text{span}\{y_{k+1}, ..., y_n\}^{\perp} + \text{dim} \ \text{span}\{u_k, ..., u_n\} \geq n + 1
    \end{align*}
    So the intersection $\text{span}\{y_{k+1}, ..., y_n\}^{\perp} \cap \text{span}\{u_k, ..., u_n\}$ must have some nonzero vectors. Let $w \neq 0$ be one such vector. We have
    \begin{align*}
        w \perp u_1, ..., u_{k-1},
    \end{align*}
    and so by Rayleigh-Ritz,
    \begin{align*}
        \frac{w^* A w}{w^* w} \geq \lambda_k \implies \max_{\substack{w \neq 0 \\ w \perp y_{k+1} ,..., y_n \\ w \perp u_1, ..., u_{k-1}}} \frac{w^* A w}{w^* w} \geq \lambda_k
    \end{align*}
    This also means that
    \begin{align*}
        \phi_{\max}(y_{k+1}, ..., y_n) = \max_{\substack{x \neq 0 \\ x \perp y_{k+1}, ..., y_n}} \frac{x^* A x}{x^* x} \geq \max_{\substack{w \neq 0 \\ w \perp y_{k+1}, ..., y_n \\ w \perp u_1, ..., u_{k-1}}} \frac{w^* A w}{w^* w} \geq \lambda_k
    \end{align*}
    and thus we have found a lower bound for $\phi_{\max}(y_{k+1}, ..., y_n)$. Setting $y_{k+1} = u_{k+1}, ..., y_n = u_n$, we achieve this lower bound, and thus
    \begin{align*}
        \min_{y_{k+1},..., y_n} \phi_{\max}(y_{k+1}, ..., y_n) = \min_{y_{k+1},..., y_n} \max_{\substack{x \neq 0 \\ x \perp y_{k+1}, ..., y_n}} \frac{x^* A x}{x^* x} &= \lambda_k\\
    \end{align*}
    as desired.
\end{proof}

\begin{thm}[Weyl]
    Let $A, B \in M_n$ be Hermitian. Then, for all $k = 1, ..., n$
    \begin{align*}
        \lambda_k(A) + \lambda_1(B) \leq \lambda_k(A + B) \leq \lambda_k(A) + \lambda_n(B)
    \end{align*}
    where $\lambda_i(A)$ is the $i$-th smallest eigenvalue of $A$.
\end{thm}

\begin{coro}
    Let $A, B \in M_n$ be P.S.D.. Then for all $k = 1, ..., n$,
    \begin{align*}
        \lambda_k(A) \leq \lambda_k(A + B)
    \end{align*}
\end{coro}

\begin{rem}
    For $a \in \{-1, 1\}, y \in \C^n \backslash \{0\}$, $ayy^*$ is a rank 1 Hermitian matrix. Any rank 1 Hermitian matrix can be written as such.
\end{rem}
\begin{thm}[Interlacing I]
    For $A$ Hermitian, $a \in \{-1, 1\}, y \in \C^n \backslash \{0\}$, then for all $k$,
    \begin{align*}
        \lambda_k(A + ayy^*) \leq \lambda_{k+1}(A) 
    \end{align*}
\end{thm}

\begin{thm}[Interlacing II]
    Let $A \in M_n$ be Hermitian, $B \in M_r$ be a principal submatrix of $A$. Then for all $k$ such that $1 \leq k \leq r$,
    \begin{align*}
        \lambda_k(A) \leq \lambda_k(B) \leq \lambda_{k+n-r}(A)
    \end{align*}
\end{thm}

\begin{coro}
    If $A \in M_n$ is Hermitian, and $a_{ii}$ is on the diagonal, then
    \begin{align*}
        \lambda_1(A) \leq a_{ii} \leq \lambda_n
    \end{align*}
\end{coro}

\subsection*{Application: multivariate statistics}

In the interest of time, and the fact that there are many resources on this subject, we may not go over the statistics example fully in class. However, here is the setup and the questions, and you can answer them on your own and discuss in office hours!

Let $x: \Omega \rightarrow \R^d$ and $x \in \R^d$ both represent a $d$-dimensional random variable and its realization. The distinction should be clear from context. Let $\EV[x] = \mu$ be the mean and $\text{Cov}[x] = \EV[(x - \mu)(x - \mu)^T] = \Sigma$ be the (Hermitian) covariance matrix.
\begin{enumerate}
    \item Prove that $\Sigma$ is P.S.D.. Why is it not P.D. in general?
    \item Interpret the eigenvectors, eigenvalues, and diagonal matrix $D$.
    \item Let $x \sim \mathcal{N}(\mu, \Sigma)$. Prove that $\Sigma$ is in fact P.D.. Plot the probability density function for $d = 2$ and use the optimization example to interpret the shape.
    \begin{itemize}
        \item Why does a large eigenvalue $\lambda_k$ correspond to a large spread in $u_k$ (meaning large value of $\text{Var}(||\text{proj}_{u_k}(x)||)$)?
        \item For what values of the spectrum of $\Sigma$ are the level sets spherical versus oblong?
        \item When are the major axes of the level sets aligned with the coordinate axes?
    \end{itemize}
    \item If $x \sim \mathcal{N}(\mu, \Sigma)$ and $\Sigma = UDU^T$, what is the distribution of $z = U^T x$? Interpret $z$.
    \item Let $X \in M_{n, d}(\R)$ be a data matrix, where each row $x^T_i$ is an independent observation of $x$. Assume for simplicity that $\EV[x] = 0$. Finally, let $\frac{1}{n} X^T X = \hat{U} \hat{D}\hat{U}^T$ be an estimate of the unitary diagonalization of $\Sigma$. Consider two dimension reduction routines.
    \begin{itemize}
        \item Represent the data as $Z = X\hat{U}$, i.e. the principal components of $X$. Then, drop $d - r$ columns by some method.
        \item Drop columns from the {\it original} data matrix $X$ to $X_r$, then orthogonalize the reduced covariance matrix $\frac{1}{n} X_r^T X_r = \hat{U}_r \hat{D}_r\hat{U}^T_r$. Represent the reduced data as $Z_r = X_r \hat{U}_r$.
    \end{itemize}
    What is the conceptual difference between these two methods, that is, orthogonalizing and then dropping columns (of which PCA is an example) or vice versa? (Hint: use Interlacing II.)
\end{enumerate}

\section{October 22, 2019}

\subsection*{Exercises in section}

\begin{enumerate}
    \item What can be said about the diagonal elements of a skew Hermitian matrix?
    \item Let $A \in M_n$ be Hermitian. Prove that $A = A_+ + A_-$, where
    \begin{itemize}
        \item $A_+$ is P.S.D. and $A_-$ is N.S.D.,
        \item rank($A$) = rank($A_+$) + rank($A_-$), and 
        \item $A_+ A_- = A_+ A_- = 0$.
    \end{itemize}
    \item Prove that
    \begin{itemize}
        \item if $A$ is Hermitian, then $A^2$ is P.S.D.
        \item if $A$ is skew Hermitian, then $A^2$ is N.S.D.
    \end{itemize}
    \item Let $\mathcal{H}_n$ be the Hermitian matrices in $M_n$. $A \succeq B$ for $A, B \in \mathcal{H}_n$ if $A - B$ is P.S.D.. Prove that this is a partial ordering on $\mathcal{H}_n$, in that
    \begin{itemize}
        \item $A \succeq A$,
        \item $A \succeq B$ and $B \preceq A \iff A = B$, and
        \item $A \succeq B$ and $B \succeq C \implies A \succeq C$.
    \end{itemize}
    Why is $\mathcal{H}_n$ not totally ordered?
    \item Let $A \in M_n$ be Hermitian with exactly one positive and one negative eigenvalue. Explain why $\lambda_2(A) \geq 0$ and $\lambda_{n-1}(A) \leq 0$ with equality if and only if $n > 2$.
    \item Prove that the inner minimization and maximization of Courant-Fishcher always exists. That is, for any $A \in M_n$, $y_{1}, ..., y_{m}$ with $m < n$,
    \begin{align*}
        \inf_{x \neq 0, \ x \perp y_{1}, ..., y_{m}} \frac{x^* A x}{x^*x} \ \text{ and } \ \sup_{x \neq 0, \ x \perp y_{1}, ..., y_{m}} \frac{x^* A x}{x^*x}
    \end{align*}
    are achieved and can be replaced by minimum and maximum, respectively. You can take for granted that the matrix $z^* A z$ is continuous in $z$, with respect to the metric $d(z_0, z_1) = ||z_0 - z_1||$ where $||\cdot||$ is the Euclidean norm. We will be able to show this rigorously in Chapter 5.)
\end{enumerate}

We'll give a proof for the infimum in part (f), as to start getting used to the real analysis style proofs. 
\begin{proof}
First we note that
\begin{align*}
    \frac{x^* A x}{x^*x} = \frac{x^* A x}{||x||^2} = \left(\frac{x}{||x||}\right)^* A \left(\frac{x}{||x||}\right)
\end{align*}
$\frac{x}{||x||}$ is a unit vector, so we can just search values of $z^* A z$ over unit vectors $z$. Similarly, $x \perp y_{1}, ..., y_{m}$ if and only if $\frac{x}{||x||} \perp y_{1}, ..., y_{m}$, as scaling the length does not affect orthogonality (for non-zero vectors). Thus,
\begin{align*}
    \inf_{x \neq 0, \ x \perp y_{1}, ..., y_{m}} \frac{x^* A x}{x^*x} = \inf_{||z||_2 = 1, \ z \perp y_{1}, ..., y_{m}} z^* A z
\end{align*}
Using the hint that $z^* A z$ is continuous, we can show the existence of the infimum if the search space is compact. It is clear that the unit sphere $\{z : ||z|| = 1\}$ is compact. On the other hand, the set $\{z: z \perp y_{1}, ..., y_{m}\}$ is a subspace of dimension $l \leq m$. The search space is the intersection of these two sets. See Figure \ref{fig:sphere} for an example.
\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/sphere-and-plane.png}
    \caption{Let $O$ be the origin, and $y$ 
    (not shown) be a vector normal to the plane. Then, $A$ and $B$ are two points in the intersection. They are both of unit length, and both orthogonal to $y$.}
    \label{fig:sphere}
\end{figure}
The subspace is represented by a hyperplane that crosses the origin. The intersection of an $l$-dimensional subspace and an $n$-dimensional unit sphere is an $l$-dimensional unit sphere. This is still a compact set, completing the proof that the inf (and sup) exist, which can now be replaced by min (and max).
\end{proof}

Pointers for Homework 4:
\begin{enumerate}
        \item What relationship can you advance about the set of Hermitian matrices and the set of skew Hermitian matrices?
        \item If $x \in \R^n$ majorizes $y \in \R^n$, what can you say about $x_1$ and $y_1$ (by definition) and about $x_n$ and $y_n$ (by inspection)?
        \item What can you say about any commuting matrices? If they are normal?
        \item Remember that in constructing principle submatrices, the undeleted rows/columns need not be contiguous. That is, for $n = 10$, I can delete rows/columns 1 - 3 and 5 - 7. This generates a submatrix of rows/columns 4, 8, 9, and 10.
        \item The leading principle submatrices $A_1, A_2, ..., A_n$ of $A$ are by definition principle submatrices of $A$. What else is true about them?
        \item For Problem 7, you should have a clear and specific analogue for Rayleigh-Ritz, Courant-Fischer, Interlacing I and II (and consequences), Weyl's theorem, and the majorization results.
\end{enumerate}

\section{October 29, 2019}

The theme of this section is understanding the singular value decomposition (SVD) of common data matrices. In statistics and machine learning, the data matrix (or design matrix) $X \in M_{n, d}(\R)$ has {\bf rows} $x^{(1)}, ..., x^{(n)} \in \R^d$ where each $x^{(i)}$ is an observation of some random vector
\begin{align*}
    x: \Omega \rightarrow \R^d
\end{align*}
with mean
\begin{align*}
    \mu = \EV[x] \in \R^d
\end{align*}
and covariance matrix
\begin{align*}
    \Sigma = \Cov(x) = \EV[(x - \mu)(x - \mu)^T] = \EV[xx^T] - \EV[x]\EV[x^T]
\end{align*}
This is analogous to the variance formula for univariate $x$, i.e. $\Var(x) = \EV[x^2] - \EV[x]^2$. We will just be thinking about the real version of these objects, but for complex numbers the analysis is the same, replacing transpose with conjugate transpose. Another data matrix we will visit is a grayscale image $P \in M_{m,n}(\R)$. 

We first will compare the SVD and eigendecomposition of an arbitrary Hermitian matrix $A$. If $A = UDU^*$ and $A^* = UD^*U = UDU^*$, then
\begin{align*}
    AA^* = U D U^* U D ^* = U D^2 U^*
\end{align*}
meaning that $A$ and $AA^*$ share the same eigenvectors, with $AA^*$ having the same eigenvalues, but squared. The ``$U$" matrix of the eigendecomposition of $AA^*$ is the same as the ``$U$" of the SVD of $A$, by definition.
\begin{align*}
    A = U \Sigma V^*
\end{align*}
What about $\Sigma$? Also by definition, we have that
\begin{align*}
    \Sigma = (D^2)^{\frac{1}{2}} = |D| = D \cdot \text{sgn}(D)
\end{align*}
where the $|\cdot|$ denotes element-wise absolute value, and $\text{sgn}(\cdot)$ is the element-wise sign function. 
\begin{align*}
    \text{sgn}(t) = \begin{cases}
    +1 &\text{ if } t > 0\\
    0 &\text{ if } t = 0\\
    -1 &\text{ if } t < 0
    \end{cases}
\end{align*}
Thus we can write
\begin{align*}
    UDU^* = A = U \Sigma V^* = U D \cdot \text{sgn}(D) V^*
\end{align*}
If $A$ is invertible, then $D$ is invertible, and we can conclude by cancelling out $UD$ that
\begin{align*}
    U^* = \text{sgn}(D) V^*
\end{align*}
Thus, the rows of $V^*$ are just equal to the rows of $U^*$, down to a sign. Where $A$ has a positive eigenvalue $\lambda_i$, the rows $u_i^*$ and $v_i^*$ will agree. Where $A$ has a negative eigenvalue $\lambda_i$, then $u_i^* = -v_i^*$. If $D$ is not invertible, then there will be zeros in $\text{sgn}(D)$, but if that is the case, then it doesn't really matter what is in those rows of $U^*$ and $V^*$ (as long as they are orthonormal), as those rows will get wiped out by the zero singular values in $\Sigma$. So we can interpret this idea as ``essentially", $U^*$ and $V^*$ will agree down to signs of rows. Additionally, if $A$ is positive semidefinite (P.S.D.), then we can say that the SVD and eigendecomposition are the {\bf same} (we will omit the ``essentially" from now on).

Coming back to the random variable $x$, will first show that the covariance matrix $\Sigma$ is P.S.D.. Give any $a \neq 0 \in \R^d$, we have:
\begin{align*}
    a^T \Sigma a &= a^T (\EV[xx^T] - \EV[x]\EV[x^T]) a\\
    &= \EV[a^Txx^Ta] - \EV[a^Tx]\EV[x^Ta] \\
    &= \EV[(a^T x)^2] - (\EV[a^T x])^2 \\
    &= \Var(a^T x)
\end{align*}
This value is nonnegative for any $a$, completing the proof. We cannot say more than that, because $a^T x$ can be constant even when each coordinate of $x$ has variance. Take for example
\begin{align*}
    x = \begin{bmatrix}y \\ 1 - y \end{bmatrix}
\end{align*}
where $y \sim $ Unif(0, 1). Letting $a = [1 \ 1]^T$, we have that $a^Tx = 1$ with probability 1, i.e. $\Var(a^Tx) = 0$. So if the covariance matrix was P.D., what would that imply? It means that no dimensions of $x$ are linearly dependent. For those taking Statistical Theory I (EN.553.730), this is the concept behind a rank $d$ exponential family (see Section 1.6 of \citet{bickel}), in that the sufficient statistic $T$ must have P.D. covariance.

Similarly, if we expand to the full data matrix $X$, every column of $X$ contains independent copies of observations from each dimension. If $\Sigma$ is P.D., then no column of $X$ can be represented as a linear combination of the others. This reveals a goal - if the data matrix did have columns that were expressible as linear combinations of the others, we might want to drop those columns, reducing the dimension of our dataset, which might have benefits for statistical inference. This can be done harmlessly if $\Sigma$ is not P.D. and has some of its eigenvalues (hence singular values) set to zero. What if there is an eigenvalue $\lambda_j$ of $\Sigma$ such that
\begin{align*}
    \lambda_j = \epsilon \approx 0
\end{align*}
for some small $\epsilon > 0$? This means that there is a dimension of $x$ (hence a column of $X$) that is {\bf approximately} a linear combination of the others. We still might be able to benefit statistically by dropping such a column from out dataset. As it turns out, in any dataset, we can have columns highly correlated with one another. This means that the non-diagonal entries of $\Sigma$ can be far from zero. The next question is whether there exists a basis to represent the dimensions of the data are uncorrelated. We know that $\Sigma$ is P.S.D., so we can write
\begin{align*}
    \Sigma = UDU^T
\end{align*}
What is the interpretation of $D$?
\begin{align*}
    D &= U^T \Sigma U\\ 
    &= U^T (\EV[xx^T] - \EV[x]\EV[x^T]) U\\
    &= \EV[U^T x(U^T x)^T] - \EV[U^T x]\EV[(U^T x)^T]\\
    &= \Cov(U^T x)
\end{align*}
The random vector $U^T x$ is just $x$ in a rotated basis, but in this basis, all of the dimensions of $U^T x$ are uncorrelated! ($U^Tx$ is a transform of $x$ into the basis of $u_1, ..., u_d$, the columns of $U$, because $U$ is real orthogonal.) Now, we can consider the random variable $z = U^T x$ with linearly independent coordinates. The $j$-th diagonal of $D$ is $\lambda_j$, we have
\begin{align*}
    \lambda_j = \Var(z_j) = \Var(u_j^T x)
\end{align*}
because
\begin{align*}
    z = U^T x = \begin{bmatrix}
    u_1^T x\\
    \vdots\\
    u_d^T x
    \end{bmatrix}
\end{align*}
We can also write
\begin{align*}
    x = Uz = z_1 u_1 + ... z_d u_d = (u_1^T x)u_1 + ... + (u_d^T x)u_d = \sum_{j=1}^d (u_j^T x)u_j
\end{align*}
as the orthogonal projection of $x$ onto the orthonormal basis $\{u_1, ..., u_d\}$. We are interested in recovering the matrix $Z = XU \in M_{n,d}(\R)$, the orthogonalized representation of $X$. Other than the benefit of being able to observe the components of $x$ that are uncorrelated, we can drop columns that have low values of $\lambda_j$, as they may not describe the main patterns in the data. This linear dimension reduction technique is known as principle component analysis (PCA), and can be read about in detail \href{https://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch18.pdf}{here}.

We generally do not have access to the true distribution generating $x$, and therefore do not have $\Sigma$ on hand. Let's assume that $\EV[x] = 0$. (We can achieve this by subtracting the sample mean from each point.) Then, a natural estimate of the covariance matrix is
\begin{align*}
    \hat{\Sigma} = \frac{1}{n} X^T X
\end{align*}
\begin{exc}
    Prove that the estimate $\hat{\Sigma}$ converges in probability to $\Sigma$. That is, prove that for all $\epsilon > 0$
    \begin{align*}
        P[||\hat{\Sigma} - \Sigma||_F > \epsilon] \rightarrow 0
    \end{align*}
    as $n \rightarrow \infty$. You can use the Weak Law of Large Numbers.
\end{exc}
\begin{exc}
    Similarly, show trivially that the eigenvalue vector $\vec{\lambda}(\hat{\Sigma})$ converges in probability to the true eigenvalue vector $\vec{\lambda}(\Sigma)$. You can use that the eigenvalues are continuous over $M_n$.
\end{exc}
\begin{exc}
    Check that $\hat{\Sigma}$ is P.S.D. as well. Thus, $\hat{\Sigma}$ can be eigendecomposed with nonnegative eigenvalues.
\end{exc}
From the previous exercise, we have that
\begin{align*}
    \hat{\Sigma} = \hat{U} \hat{D} \hat{U}^T
\end{align*}
and we can estimate the {\it loading} matrix
\begin{align*}
    \hat{Z} = X \hat{U}
\end{align*}
From here, columns can be dropped by any method. While this checks out theoretically, from a numerical viewpoint, there are some shortcomings. Both the computation of $\hat{\Sigma}$ as well as its eigendecomposition are notoriously expensive operations. Additionally, eigendecomposition is less numerically stable than singular value decomposition. (You should believe that taking the eigendecomposition of $AA^*$ is not how the SVD of $A$ is typically computed by linear algebra libraries.) Is there a way to compute $\hat{Z}$ without either of the above steps? The answer is in the SVD. Consider the SVD of the matrix below.
\begin{align*}
    \tfrac{1}{\sqrt{n}} X^T = \hat{U} \hat{S} \hat{V}^T 
\end{align*}
This matrix is chosen so that ``$\hat{U}$" is the same ``$\hat{U}$" that we discussed before - an estimate of the eigenvectors of $\Sigma$, as
\begin{align*}
    \tfrac{1}{\sqrt{n}} X^T \left(\tfrac{1}{\sqrt{n}} X^T\right)^T = \tfrac{1}{n} X^T X = \hat{U} \hat{D} \hat{U}^T 
\end{align*}
and this the $\hat{U}$ used in the SVD of $\tfrac{1}{\sqrt{n}} X^T$. Then we can take the transpose of the SVD $\tfrac{1}{\sqrt{n}} X^T$, which is the SVD of $\tfrac{1}{\sqrt{n}} X$.
\begin{align*}
    \tfrac{1}{\sqrt{n}} X = \hat{V} \hat{S}^T \hat{U}^T
\end{align*}
and multiply both sized by $\hat{U}$.
\begin{align*}
    \tfrac{1}{\sqrt{n}} X \hat{U} = \hat{V} \hat{S}^T \hat{U}^T \hat{U} = \hat{V} \hat{S}^T
\end{align*}
Thus, we have
\begin{align*}
    \hat{Z} = X \hat{U} = \sqrt{n} \hat{V} \hat{S}^T
\end{align*}
The orthogonalized $\hat{Z}$ can be computed fully by the SVD of $X$! You may have not seen the $\sqrt{n}$ factor before, but this is only written so that the $M_d$-valued matrix in the SVD of $X$ is the same as the eigenvector matrix of $\hat{\Sigma}$. Multiplying the entire dataset by a number will usually not affect the result of statistical inference.

Departing from the statistical example to a very simple image analysis example, we can consider an $m$-by$-n$ grayscale image $P$. If we compute its SVD, rather than writing in matrix form, we can write $P$ as the sum of scaled rank 1 matrices.
\begin{align*}
    P = \sum_{i=1}^{\min(m,n)} \sigma_i u_i v_i^T
\end{align*}
where $u_i$ and $v_i$ are the $i$-th columns of $U$ and $V$, respectively, and $\sigma_i$ is the $i$-th ``diagonal" of $\Sigma$. Thus, $P$ is generated by overlaying many rank 1 layers. It is conceivable that the largest singular values correspond to layers containing main effects such as objects and shapes, moderate singular values contain details such as texture, and near-zero singular values correspond to noise layers. The image can be written in a compressed, low-rank format with virtually no loss of discernable information. At this \href{http://timbaumann.info/svd-image-compression-demo/}{site}, you can drag a slider that determines the number of singular values to retain in various images. For many of the images, hundreds of singular values can be dropped without much change in the image. These are just two among the many applications of the SVD.

\section{November 5, 2019}

Exam 2 is on Friday, November 8, 2019. An enumeration of the topics in Chapter 4 can be found at the end of the seventh lecture module (Hadamard's Inequality). As for Chapter 7, the main topics covered were:
\begin{itemize}
    \item Construction of the SVD.
    \item Properties and consequences of the SVD.
    \begin{itemize}
        \item Range, rank, nullspace from SVD.
        \item Uniqueness of singular values, not of singular vectors.
        \item $||A||_{2,2} = \sigma_1(A)$
    \end{itemize}
    \item Polar decomposition.
    \item Generalized inverse properties.
    \item Existence and uniqueness of Moore-Pensrose generalized inverse.
    \item Generalized inverses and linear systems.
\end{itemize}

{\it Copies of Homework 4 and 5 solutions distributed.}

\section{November 12, 2019}

The theme of this section will be to convince you that it is interesting and useful to use inner products, norms, and metrics to generalize our notions of orientation, length, and distance beyond three-dimensional Euclidean space (or more generally, $\K^n$ where $\K = \R$ or $\C$).

\subsection{Infinite bases}

With inner products and norms, we can induce metrics. Thus, every inner product space (IPS) and normed linear space (NLS) is a metric space, and all of the theory we developed in the beginning of the course applies. Specifically, with metrics we have limits and convergence of sequences, and with convergence we can define truly infinite bases. When we were working in just vector spaces without additional structure, we defined a basis $\B$ of vector space $V$ as a (possibly infinite) set of vectors such that
\begin{itemize}
    \item Any finite subset of vectors $b_1, ..., b_k \in \B$ are linearly independent.
    \item For all $v \in V$, $v = \alpha_1b_1 + ... +\alpha_k b_k$ for some finite subset of vectors $b_1, ..., b_k \in \B$ and $\alpha_1, ..., \alpha_k \in \K$.
\end{itemize}
This is known as a {bf Hamel or algebraic basis}. Now, assume that there is an inner product and hence norm on Hilbert space $V$, and there is a countable set of vectors $\B = \{b_1, b_2, ...\}$. We can say that:
\begin{align*}
    v = \sum_{i=1}^\infty \alpha_i b_i 
\end{align*}
if
\begin{align*}
    \left|\left|\sum_{i=1}^n \alpha_i b_i - v\right|\right| \rightarrow 0
\end{align*}
as $n \rightarrow \infty$. Precisely, this is what is really meant by the notation $v = \sum_{i=1}^\infty \alpha_i b_i$, and nothing else. This is an infinite linear combination. We say that $\B$ is a linearly independent set of vectors if
\begin{align*}
    \sum_{i=1}^\infty \alpha_i b_i = 0 \implies \alpha_i = 0 \ \forall i
\end{align*}
Using these ideas, a countable basis for a normed linear space can be defined using the new definitions of linear combinations and linear independence. This basis is called a {\bf Hilbert basis}, in that convergence holds with respect to the induced distance. What is a canonical example of a countable basis for a NLS $V$? $V$ must be infinite-dimensional, as any finite dimensional linear space over $\K$ is isomorphic to $\K^n$, which has a finite basis. A common example is the Fourier modes $b_k(\cdot) = e^{2\pi i k f_0 \cdot}$ where $f_0$ is the fundamental frequency in Hz and $k \in \Z$ for some class of periodic functions $\{f(\cdot)\}$ which we will explore later.

\subsection*{The space $L^2([0, 1))$}

Now, we will give an example of infinite-dimensional vector space with a norm and inner product. Consider the set of functions
\begin{align*}
    L^2([0,1)) = \{f : \int_{[0,1]} |f(t)|^2 d\mu < \infty \}
\end{align*}
where the $\int \cdot \ d\mu$ notation on the right refers to Lebesgue integration (don't worry if you haven't seen this before, just think if it as the integration you are used to). This is actually a Hilbert space, with inner product
\begin{align*}
    \ip{f}{g} = \int_0^1 f(t)\overline{g(t)} d\mu
\end{align*}
and norm
\begin{align*}
    ||f|| = \sqrt{\ip{f}{f}} = \sqrt{\int_0^1 |f(t)|^2 d\mu}
\end{align*}
\begin{exc}
    Prove that $L^2([0, 1))$ is a vector space.
\end{exc}
$L^2([0,1))$ is only complete with this choice of norm. It is not a Hilbert space with respect to say $||f||_1 = \int_0^1 |f(t)| d\mu = $ Note that this is a space of {\bf equivalence classes of functions} rather than functions, as we can have $||f-g|| = 0$ but $f \neq g$ if $f$ and $g$ disagree on a set of ``measure" zero. Thus, we get around this by letting each element of this space be a set of functions that are ``essentially" the same. We identify these functions with one another. We will drop the ``essentially" and keep this point in the back of our heads. Finally, we are only considering functions on $[0, 1)$, as the function could be considered periodic by copying the values of the function on $[0, 1)$ and pasting them onto the intervals $[1, 2), [2,3)$ etc. Similarly, functions without period 1 can be scaled to fit on this interval, and the analysis remains the same.
\begin{exc}
    Confirm that the inner product proposed above adheres to the four properties for complex inner products.
\end{exc}

\subsection*{The Fourier modes}

Consider the class of functions of the following form:
\begin{align*}
    u_k(t) = e^{2\pi i k t}
\end{align*}
for $k \in \Z$. These are called the Fourier modes. It turns out that these functions form an orthonormal (Hilbert) basis for $L^2([0,1))$.
\begin{exc}
    Prove that $\B = \{..., u_{-1}(\cdot), u_0(\cdot), u_1(\cdot), ...\}$ is an orthonormal set of vectors, in that $\ip{u_k}{u_l} = 0$ for $k \neq l$ and $\ip{u_k}{u_l} = 1$ for $k = l$.
\end{exc}
Every $f \in  L^2([0, 1))$ can be represented in this basis. How do we usually project a vector $x$ into an orthonormal bases $u_1, ..., u_n$ in linear algebra? Let $U = [u_1, ..., u_n]$ be the unitary matrix generated by column binding the basis vectors.
\begin{align*}
    x = UU^*x = \sum_{k=1}^n (u_k^*x) u_k = \sum_{k=1}^n \ip{x}{u_k} u_k
\end{align*}
In $L^2([0,1))$ with the basis of the Fourier modes, we can write what is known as the {\bf analysis equation} in signal processing:
\begin{align*}
    a_k &= \int_0^1 f(t) \overline{e^{2\pi i k t}} d\mu\\
    &= \int_0^1 f(t) e^{-2\pi i k t} d\mu\\
    &= \ip{f}{u_k}
\end{align*}
The Fourier expansion of the function $f$ is given by
\begin{align*}
    f(t) = \sum_{k \in \Z} a_k e^{2\pi i k t} = \sum_{k \in \Z} \ip{f}{u_k} u_k(t)
\end{align*}
which is nothing but the {\bf orthogonal projection} of $f$ onto the Fourier basis! So, to understand the Fourier expansion the intuition of projecting vectors onto the subspace spanned by orthonormal vectors (as in Figure \ref{fig:proj}) suffices. Another central concept of linear algebra is eigenvectors and eigenvalues. Do they have any significance in this setting?

Consider the set of operators $H: L^2([0,1)) \rightarrow L^2([0,1))$ that are linear and time-invariant. Specifically, for $f, g \in L^2([0,1))$ and $a, b \in \K$:
\begin{align*}
    H(af + bg) = aHf + bHg
\end{align*}
and
\begin{align*}
    H[f(t-t_0)] = (Hf)(t - t_0)
\end{align*}
where $f(t - t_0)$ shifts the function rightward by $t_0$. These are called the {\bf LTI systems} in signal processing. They are mappings that operate on function spaces, and being linear, we can consider eigenvectors of these operators. These eigenvectors turn out to be the Fourier modes themselves. Thus,
\begin{align*}
    H(e^{2\pi i k t}) = \lambda_k e^{2\pi i k t}
\end{align*}
for some eigenvalue that depends only on $k$. Consider an LTI system that is also continuous with respect to the $L^2$ norm (make sure you understand what this means). We can use these eigenvalues and the Fourier expansion to evaluate the output of any LTI system, as
\begin{align*}
    H[f(t)] &= H\left(\sum_{k \in \Z} a_k e^{2\pi i k t}\right)\\
    &= H\left(\lim_{N \rightarrow \infty}\sum_{k = -N}^N a_k e^{2\pi i k t}\right)\\
    &= \lim_{N \rightarrow \infty} H\left(\sum_{k = -N}^N a_k e^{2\pi i k t}\right)\\
    &= \lim_{N \rightarrow \infty} \sum_{k = -N}^N a_k H(e^{2\pi i k t})\\
    &= \lim_{N \rightarrow \infty} \sum_{k = -N}^N a_k \lambda(k) e^{2\pi i k t}\\
    &= \sum_{k \in \Z} a_k \lambda_k e^{2\pi i k t}
\end{align*}
In this basis, in order to apply the operator, we need only scale the Fourier coefficients $a_k$ by $\lambda_k$. This should sound familiar, in that it is exactly the benefit of working with a diagonalizable matrix. If $A = SDS^{-1}$, the linear transformation described by $A$ reduces to just scaling in the basis given by the columns of $S$. 

The eigenvectors of LTI systems form a basis for $L^2([0,1)$ (akin to $n$ linearly independent eigenvectors of a matrix), so you we say that the LTI systems are {\bf diagonalizable}, with ``diagonal entries" (i.e. eigenvalues) given by the $\lambda_k$. In fact, because those eigenvectors are orthonormal, the LTI systems are unitarily diagonaliable (normal) operators.

What exactly is unitary here? Because matrices are called unitary, there must be a linear operator that is unitary. This is none other than that which projects the vector $f$ onto the orthonormal basis described by the Fourier modes - or, the Fourier expansion. Moreover, because the LTI systems are diagonalizable with the same eigenvectors, they are simultaneously diagonalizable. You should then not be surprised that LTI systems commute, in that
\begin{align*}
    HLf = LHf
\end{align*}
for operators $H$ and $L$. There is a much easier way to show they commute by using the convolution with the impulse response. However, this direction was intended to draw the connection to matrices.

\subsection*{Another example of Hilbert space}

It is of note that Fourier analysis arose to solve problems of heat transfer and oscillatory motion, and the $L^2$ theory is a linear algebraic way of understanding ideas that had developed in different ways. Another field that can be imbued with linear algebra intuition is statistics. Consider the set of random variables $X$ with finite second moment $\EV[X^2] < \infty$. This forms a Hilbert space with respect to the covariance inner product.
\begin{align*}
    \ip{X}{Y} = \Cov(X,Y)
\end{align*}
\begin{exc}
    Show that space of random variables with finite second moment is a vector space.
\end{exc}
\begin{exc}
    Show that the covariance operator is an inner product.
\end{exc}
\begin{exc}
    Prove that the set of finite second moment random variables is complete with respect to the metric induced by this inner product.
\end{exc}
\begin{exc}
    How is the finite second moment condition for random variables related to the square-integrable condition for $L^2$?
\end{exc}
\begin{exc}
    Can you come up with an orthonormal basis for this Hilbert space?
\end{exc}
What is the norm of a random variable then?
\begin{align*}
    ||X|| = \sqrt{\ip{X}{X}} = \sqrt{\Cov(X,X)} = \sqrt{\Var(X)}
\end{align*}
The ``length" of a random variable is given by its standard deviation, and random variables are orthogonal if they are uncorrelated. With inner products, we have the ability to describe angles between vectors. A common formula regarding inner products
\begin{align*}
    \ip{u}{v} = ||u|| \cdot ||v|| \cos(\theta)
\end{align*}
where $\theta$ is the angle between vectors. Applying this to random variables, we have:
\begin{align*}
    \cos(\theta) &= \frac{\ip{X}{Y}}{||X|| \cdot ||Y||} = \frac{\Cov(X,Y)}{\sqrt{\Var(X) \Var(Y)}} = \text{Cor}(X, Y)\\
    \implies \theta &= \arccos \text{Cor}(X, Y)
\end{align*}
Correlation is just the cosine of the angle between random variables - consequently, it is bounded between -1 and 1.
\begin{exm}
    Consider a list of identically distributed random variables $Z_1, Z_2, Z_3$ such that $\text{Cor}(Z_i, Z_j) = \rho$ for all $i \neq j$. What are the bounds on $\rho$?\\
    
    We will use the geometric intuition of the Hilbert space to work through this problem. Because there are only three vectors, we can consider them to be lying on the finite-dimensional subspace spanned by each of them. We can represent these vectors in an orthonormal basis by Gram-Schmidt (think of an orthonormal Hilbert basis for this space). This subspace is isomorphic to $\R^3$, and without loss of generality we can consider the standard basis.
    
    Given any set of three vectors in Euclidean space that all have the same angle $\theta$ separating one another, what are the bounds on that angle? The angle could be 0, in that all vectors are pointing the same direction. Maximally, the angle could be $\frac{2\pi}{3}$ (see Figure \ref{fig:three_vecs}). The correlation is the cosine of this angle, giving us the interval
    \begin{align*}
        [\cos(\tfrac{2\pi}{3}), \cos(0)] = [-\tfrac{1}{2}, 1]
    \end{align*}
    \begin{figure}
        \centering
        \includegraphics[width=0.5\linewidth]{figures/three_vecs.png}
        \caption{The three red vectors could be spaced maximally at an angle of $\theta = \frac{2\pi}{3}$ from one another, and minimally at $\theta = 0$.}
        \label{fig:three_vecs}
    \end{figure}
    How can three random variables all have negative correlation with one another? Consider any $X_1, X_2, X_3$ and define
    \begin{align*}
        Z_1 &= X_1 - \overline{X}\\
        Z_2 &= X_2 - \overline{X}\\
        Z_3 &= X_3 - \overline{X}
    \end{align*}
    where $\overline{X}$ is the sample mean of $X_1, X_2, X_3$. To maintain the same average, if one goes up, the others necessarily go down, explaining the pairwise negative correlation.
\end{exm}
\begin{exm}
    A much more general question can be asked (which is also my favorite statistics question). There are three random variables $X, Y, Z$ with correlations $\rho_{XY}, \rho_{YZ}, \rho_{XZ}$. What is the full set of values in $[-1,1]^3$ that $(\rho_{XY}, \rho_{YZ}, \rho_{XZ})$ can take on?\\ 
    
    Without loss of generality, we can assume that $X$, $Y$, and $Z$ have unit variance, as that will not change the correlations. We can phrase this as ``what are the bounds on $\rho_{XZ}$ given $\rho_{XY}$ and $\rho_{YZ}$"? We can write 
    \begin{align*}
        X &= \ip{X}{Y}Y + O_Y^X = \rho_{XY}Y + O_Y^X\\
        Z &= \ip{Z}{Y}Y + O_Y^Z = \rho_{ZY}Z + O_Y^Z
    \end{align*}
    where $O_Y^X$ and $O_Y^Z$ are orthogonal to $Y$ (uncorrelated with $Y$). This is an orthogonal projection on $Y$ and its orthogonal space. Then
    \begin{equation}
        \rho_{XZ} = \ip{X}{Z} = \ip{\rho_{XY}Y + O_Y^X}{\rho_{ZY}Y + O_Y^Z} = \rho_{XY}\rho_{ZY} + \ip{O_Y^X}{O_Y^Z}
        \label{eqn:corr}
    \end{equation}
    To handle $\ip{O_Y^X}{O_Y^Z}$, observe that
    \begin{align*}
        1 = \ip{X}{X} = \ip{\rho_{XY}Y + O_Y^X}{ \rho_{XY}Y + O_Y^X} = \rho_{XY}^2 + \ip{O_Y^X}{O_Y^X}
    \end{align*}
    meaning that
    \begin{align*}
        \ip{O_Y^X}{O_Y^X} = 1 - \rho_{XY}^2
    \end{align*}
    Similarly,
    \begin{align*}
        \ip{O_Y^Z}{O_Y^Z} = 1 - \rho_{ZY}^2
    \end{align*}
    We can then use the Cauchy-Schwarz inequality to see that
    \begin{align*}
        |\ip{O_Y^X}{O_Y^Z}| &\leq \sqrt{\ip{O_Y^X}{O_Y^X} \cdot \ip{O_Y^Z}{O_Y^Z}}\\
        &= \sqrt{(1 - \rho_{XY}^2)(1 - \rho_{ZY}^2)}
    \end{align*}
    Finally, we have from \ref{eqn:corr} that
    \begin{align*}
        \rho_{XZ} \in \left[\rho_{XY}\rho_{ZY} - \sqrt{(1 - \rho_{XY}^2)(1 - \rho_{ZY}^2)}, \rho_{XY}\rho_{ZY} + \sqrt{(1 - \rho_{XY}^2)(1 - \rho_{ZY}^2)}\right]
    \end{align*}
    concluding the problem. Similar bounds can be placed on the other correlations as well. I would credit this answer entirely to this \href{https://math.stackexchange.com/questions/284877/correlation-between-three-variables-question}{user}, but there are some errors in their explanation of Hilbert space as well as some incorrect assumptions. (Can you find them?)
\end{exm}

There are many other examples in which working in a Hilbert space allows us to use geometry to thinking about our problems in a deeper way, such as the Reproducing Kernel Hilbert Space (RKHS) theory in machine learning (see the Appendix of \citet{nonpar}), wavefunctions and observables in quantum mechanics (Chapter 3 of \citet{griffiths}), and Section 3.1 of \citet{fourier} for a detailed version of the Fourier analysis ideas. If you're interested in some of these applications, definitely take Harmonic Analysis (AS.110.433) or High-Dimensional Approximation, Probability, and Statistical Learning (EN.553.738) with Dr. Mauro Maggioni!

\paragraph{Acknowledgement} I am grateful to Dr. Nicolas Charon for generously reviewing this week's notes and providing feedback (to make up for my lack of infinite-dimensional experience)!

\section{November 18, 2019}

We will start with a brief dive into condition number of stability, followed by review of Chapter 5.

\subsection*{Condition number}

We know the definition of condition number $\kappa$ of invertible $A \in M_n$ as
\begin{align*}
    \kappa_{||\cdot||}(A) = ||A||\cdot||A^{-1}||
\end{align*}
We will drop the $||\cdot||$ subscript and assume that the norm is taken to be the spectral (induced $2,2$) norm. Then
\begin{align*}
    \kappa(A) = \frac{\sigma_1(A)}{\sigma_n(A)},
\end{align*}
or the ratio of the largest singular value to the smallest. The condition number is lower bounded by 1, and we know that low condition number is synonymous with ``stability", whereas high condition number is synonymous with ``instability". Stability can refer to {\bf numerical}, {\bf statistical}, {\bf optimization}, or other perspectives. We have covered the numerical perspective in class, but you should try to interpret the other views of condition number on your own. We will not cover these in detail, as the class material is getting quite technical. As a brief note, let us first understand what types of matrices achieve the lower bound of the (spectral) conditional number.

Clearly, we must have all singular values being the same. Let $A \in M_n$ be square, and take its SVD.
\begin{align*}
    A = U\Sigma V^*
\end{align*}
Because the singular values are the same and positive (as $A$ is invertible), $\Sigma = c I$ for some $c > 0$. Then,
\begin{align*}
    A = U c I V^* = c (U V^*) = c W
\end{align*}
where $W = UV^*$ is a unitary matrix. As a restatement,
\begin{obs}
    Let $A \in M_n$. Then $\kappa(A) = 1 \iff A = cW$ for $c > 0$ and $W$ unitary.
\end{obs}
Geometrically, this means that the tranformation $x \mapsto Ax$ first rotates $x$, then stretches each elements by the same amount. It is clear then that if $A$ is P.S.D. (hence P.D. for $\kappa$ to be defined), then $U = V \implies W = I$. Taking the statistical view, we have the following observation.
\begin{obs}
    Let $x$ be a non-degenerate random variable with covariance matrix $\Sigma$. Then $\kappa(\Sigma) = 1$ if, and only if, $\Sigma = \sigma^2 I$ for some $\sigma^2 \geq 0$.
\end{obs}
Thus, we have a random variable with uncorrelated dimensions that all contribute equally to the total variance. If we plotted simulated data from such a distribution, we would expect spherical level curves. Students of statistics will understand this to be a ``nicely-behaved", stable random variable in some sense.

As for the optimization perspective, imagine a twice-differentiable function $f$ to be well-approximated by a quadratric form in the neighborhood of a local minimum (see Section \ref{sec:opti}). We already convinced ourselves that the major axes of the elliptical level curves corresponded to eigenvectors of the Hessian $\nabla^2 f$ (we made the quadratic comment so that we can assume similar/the same Hessian for any $x$ in this neighborhood). The longer axes corresponded to small eigenvalues, whereas the shorter axes corresponded to large eigenvalues (quick increase or decrease). Another observation concerning Hermitian matrices is in order.
\begin{obs}
    Let $A$ be Hermitian. Then
    \begin{enumerate}
        \item $\{\sigma_i : i = 1, ..., n\}$ = $\{|\lambda_k| : k = 1, ..., n\}$.
        \item $\kappa(A) = \frac{\max_k |\lambda_k|}{\min_k |\lambda_k|}$
    \end{enumerate}
\end{obs}
The set of singular values is the same as the set of modulus eigenvalues (check this on your own). From this, we can observe that if the condition number of the Hessian is large, then the level sets of the function will be highly oblong. On the other hand, if the condition number of the Hessian is close to 1, then the level sets will be spherical. This has implications for gradient descent-type algorithms, as seen in Figure \ref{fig:grad_desc}.
\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{figures/grad_desc.png}
    \caption{These are the level curves of a two dimensional quadratic function $f(x_1, x_2)$ with minimum at $(0,0)$. The lines $x_2 = x_1$ and $x_2 = -x_1$ are the spans of the two eigenvectors of the Hessian $\nabla^2 f$. Because this matrix is ill-conditioned, the level sets are oblong, and gradient descent wants to move in the direction posed by the spherical approximation of the Hessian, slowing down convergence.}
    \label{fig:grad_desc}
\end{figure}
To analyze this a little more closely, consider the function below on $x \in \R^d$.
\begin{align*}
    f(x) = c + b^T x + \frac{1}{2}x^T A x
\end{align*}
Given iterate $x_t$, the direction $p$ of the minimizer solves
\begin{align*}
    p = \argmin_{p \in \R^d} g(x_t)^T p + \frac{1}{2}p^T A p,
\end{align*}
in that $p = -\frac{1}{2}A^{-1} g(x_t)$ and $x_{t+1} = x_t + p$ is the minimum. The negative gradient direction $-\frac{1}{2}g(x_t)$ on the other hand solves
\begin{align*}
    p = \argmin_{p \in \R^d} g(x_t)^T p + \frac{1}{2}p^T I p
\end{align*}
So, gradient descent implicitly assumes that if the Hessian is P.D., it is well approximated by (a scalar multiple) of the identity, and would chase the minimum if that assumption was correct, also seen in \ref{fig:grad_desc}. When this assumption is not satisfied, and condition number of the Hessian is large, the iterates oscillate and are unstable in this sense. Note that these results are all with respect to the spectral norm, which is the canoncial norm when talking about condition number. A good reference for singular values, eigenvalues, and stability is actually the \texttt{MATLAB} \href{https://www.mathworks.com/content/dam/mathworks/mathworks-dot-com/moler/eigs.pdf}{documentation} on the topic.

\subsection*{Chapter 5 review}

Like before, this is a high-level enumeration of Chapter 5 topics. It's important to spend time with TAs during section and office hours fully understanding the material, as opposed to just repeating it.
\begin{itemize}
    \item Inner product, normed linear, (induced) metric spaces. (Hilbert and Banach spaces.)
    \item Cauchy-Schwarz and reverse triangle inequality.
    \item Finite dimensional normed linear spaces.
    \item Equivalent norms and convergence.
    \item Linear maps and functionals.
    \item Boundedness and continuity of linear maps.
    \item Operator norm, space of linear maps/dual space as an NLS.
    \item Special operator norms, dual norms, $l_p$ norms.
    \item Hahn-Banach theorem.
    \item Matrix norms and spectral radius.
    \item Condition number, linear systems, and stability.
    \item Matrix and vector sequences and series.
    \item Absolute, monotone norms, and the diagonal property.
    \item Monotone norms and noisey eigenvalues.
\end{itemize}

\subsection*{Chapter 5 exercises}

For the following exercise, you will need this theorem.
\begin{thm}[Riesz Representation Theorem]
    Let $\H$ be a Hilbert space over $\K$. Let $L : \H \rightarrow \K$ be a linear functional on $\H$. Then, $L$ is continuous if and only if there exists a $\phi_L \in \H$ such that $L(h) = \ip{h}{\phi_L}_{\H}$ for all $h \in \H$.
\end{thm}
We covered this theorem in class, however, we only stated and proved the theorem when $\H$ was {\bf finite-dimensional}.
\begin{exc}
    Let $V$ be a vector space over $\R$, and let $\H = \{h : V \rightarrow \R\}$ be a Hilbert space (of functions). Let $L_x$ denote the point evaluation functional, i.e. $L_x(h) = h(x)$. Prove that $L_x$ is continuous if and only if for all $x \in V$, there is a $\phi(x) \in \H$ such that $h(x) = \ip{h}{\phi(x)}$. Such an $\H$ is called a reproducing kernel Hilbert space (RKHS). 
\end{exc}

\begin{exc}
    Let $V$ be the space of quadratic functions $f(x) = ax^2 + bx + c$ on $\R$. Some up with a basis for $V$. Is $V$ finite-dimensional? If so, write the differentiation operator $D(f) = f'(x)$ as a matrix in this basis.
\end{exc}

\begin{exc}
    Of the following, which are vector spaces?
    \begin{enumerate}
        \item The set of normal matrices.
        \item The set of simultaneously diagonalizable matrices.
        \item The set of pairwise commuting matrices.
    \end{enumerate}
\end{exc}

Let $||\cdot||$ be a norm on $\C^n$.

\begin{exc}
    Show that if $||\cdot||$ is monotone, then it is absolute.
\end{exc}

\begin{exc}
    Show that if $||\cdot||$ is absolute, then $||\cdot||^D$ absolute.
\end{exc}

\begin{exc}
    Using the previous exercise, show that if $||\cdot||$ is absolute, then it is monotone.
\end{exc}

\newpage

\vspace{5mm}
\bibliography{refs}
%\bibliographystyle{IEEEtran}
\bibliographystyle{plainnat}

\newpage

\end{document}

